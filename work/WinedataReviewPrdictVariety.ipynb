{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate a basic cleaned stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._ \n",
    "\n",
    "var rawdf = (\n",
    "    spark.read.option(\"inferSchema\", \"true\")\n",
    "    .json(\"../data/winemag-data-130k-v2.json\")\n",
    "    .select(\n",
    "        trim(lower($\"variety\")) as \"variety\",\n",
    "        trim(lower($\"description\")) as \"description\")\n",
    "    .dropDuplicates(Seq(\"description\"))\n",
    "    .filter($\"variety\".isNotNull)\n",
    "    .filter(not($\"variety\".contains(\"blend\")))\n",
    "    .filter(not($\"variety\".contains(\"red\")))\n",
    "    .filter(not($\"variety\".contains(\"white\")))\n",
    "    .select($\"variety\", regexp_replace($\"description\", $\"variety\", lit(\"\")) as \"description\")\n",
    "    .select($\"variety\", regexp_replace($\"description\", \"[^\\\\p{L}\\\\p{Nd}[0-9]+]+\", \" \") as \"description\")\n",
    "    .select($\"variety\", trim(lower($\"description\")) as \"description\")\n",
    "    .cache\n",
    ")\n",
    "\n",
    "rawdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the varieties with > 3000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = (rawdf\n",
    " .groupBy($\"variety\")\n",
    " .agg(count(\"variety\") as \"count\")\n",
    " .where(\"count > 3000\")\n",
    " .join(rawdf, Seq(\"variety\"))\n",
    " .orderBy(\"variety\")\n",
    " .select($\"variety\", $\"description\")).cache\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val varietySplits = (\n",
    "    new Tokenizer()             \n",
    "    .setInputCol(\"variety\")\n",
    "    .setOutputCol(\"variety_splits\")\n",
    "    .transform(df.select(\"variety\").distinct())\n",
    "    .select(\"variety_splits\")\n",
    "    .collect()\n",
    "    .map(_.toSeq.asInstanceOf[WrappedArray[WrappedArray[String]]])\n",
    "    .flatMap(_.toSeq)\n",
    "    .flatMap(_.toSeq)\n",
    "    .toList\n",
    ")\n",
    "\n",
    "val wordsDf = (\n",
    "    new Tokenizer()\n",
    "    .setInputCol(\"description\")\n",
    "    .setOutputCol(\"words\")\n",
    "    .transform(df)\n",
    "    .select($\"variety\", $\"words\")\n",
    ")\n",
    "\n",
    "val remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filteredWords\")\n",
    "remover.setStopWords((varietySplits:::remover.getStopWords.toList).toSet.toArray)\n",
    "\n",
    "val noStopWordsDf = remover.transform(wordsDf).select($\"variety\", $\"filteredWords\"as \"words\")\n",
    "noStopWordsDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "\n",
    "val countVectorizer = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"features\")\n",
    "val countVectorizerModel = countVectorizer.fit(noStopWordsDf)\n",
    "val countVectorizerDF = countVectorizerModel.transform(noStopWordsDf)\n",
    "\n",
    "countVectorizerDF.show(3,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val indexer = (\n",
    "    new StringIndexer()\n",
    "    .setInputCol(\"variety\")\n",
    "    .setOutputCol(\"varietyIndex\"))\n",
    "\n",
    "val indexed = (\n",
    "    indexer           \n",
    "    .fit(countVectorizerDF).transform(countVectorizerDF)\n",
    "    .select($\"variety\", $\"varietyIndex\".cast(\"double\") as \"label\", $\"features\"))\n",
    "\n",
    "indexed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val Array(trainingData, testData) = indexed.randomSplit(Array(0.7, 0.3), 42L)\n",
    "\n",
    "val model = new NaiveBayes().fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(testData)\n",
    "\n",
    "val evaluator = (new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\"))\n",
    "\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
